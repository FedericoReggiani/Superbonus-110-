# -*- coding: utf-8 -*-
"""Scraping Immobiliare.ipynb

Automatically generated by Colaboratory.

"""

# Installazione Selenium e CromiumChromedriver
!pip install selenium
!apt-get update
!apt install chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin

# Import Librerie
import sys
sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')
from selenium import webdriver
from tqdm import tqdm_notebook as tqdm
import pandas as pd
import json
import pprint
import math
import time
import numpy as np

#Impostazione opzioni browser fantasma
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')

#Creiamo nuova istanza di GoogleChrome
wd = webdriver.Chrome('chromedriver',options=chrome_options)
wd2 = webdriver.Chrome('chromedriver',options=chrome_options)


# Inizializzazione variabili
wd.set_window_size(1920, 1080)
dettagli_annunci = []
link = "https://www.immobiliare.it/vendita-case/bergamo-provincia/?criterio=rilevanza"
wd.get(link)
time.sleep(2)
counter = int(wd.find_elements_by_css_selector("ul.pagination.pagination__number li.active a")[0].text) # Sempre 1
primo_giro = True
j = 0
x = 0
y = 0
inizio = 63 #pagina iniziale da cui vuoi iniziare
if inizio > 1 : primo_giro = True 
else: primo_giro = False
totale = int(wd.find_elements_by_css_selector("#listing-pagination>ul.pagination__number li:last-child>a.disabled")[0].text) # questa selezione fornisce l'ultima pagina disponibile aggiornata costantemente
n = 0.1 #visualizzi 4 pagine # Percentuale (valore compreso tra 0 e 1) di pagine che si vuole visitare
# Doppio ciclo annidato per variare pagine e annunci
tipo_ricerca = wd.find_element_by_css_selector("h1.title-listing").text
print(f"Estrazione annunci da ({link}) per ({tipo_ricerca}) - Pagina iniziale ({inizio}) - Pagina Finale ({math.ceil(totale*n)}) - ({math.ceil(totale*n) - inizio +1}) pagine visualizzate su ({totale}) totali")
while counter <= math.ceil(totale*n):   # Almeno un giro lo fa, per n > 0 ovviamente!
  if inizio != 1 and primo_giro == True:
    while counter < inizio: # Funziona!!! Però ci mette 45' (stima) per arrivare a visualizzare l'ultima pagina ed estrarre gli annunci...si può migliorare premendo sulla freccia avanti avanti e rallentare quando ci si avvicina al numero
      if (counter + 10) <= inizio : # AGGIORNAMENTO: Messo frequenza pagina di 10 in 10...così il tempo si riduce a circa 5' per visualizzare gli annunci a partire dall'ultima pagina
        wd.find_elements_by_css_selector("ul.pull-right a")[1].click()
        time.sleep(abs(1+np.random.randn()*0.2))
      else:
        wd.find_elements_by_css_selector("ul.pull-right a")[0].click()
        time.sleep(abs(1+np.random.randn()*0.2))
      counter = int(wd.find_elements_by_css_selector("ul.pagination.pagination__number li.active a")[0].text)
      print(f"Pagina iniziale in aggiornamento {counter}/{inizio}")
      primo_giro = False
  if counter == math.ceil(totale*n):
    print(f"Ultimo giro!!!")
  t = 0
  lista_annunci = wd.find_elements_by_css_selector("#listing-container li.js-row-detail")
  if len(lista_annunci)==0 : # Se la lista è vuota può esserci un errore di caricamento della pagina...si riprova
    print("Warning")
    time.sleep(5)
    lista_annunci = wd.find_elements_by_css_selector("#listing-container li.js-row-detail")
    time.sleep(5)
  if len(lista_annunci)==0 : # Se fallisce ancora, aggiorna il conteggio delle pagine sbagliate, restituisce il link alla pagina e fa uno screenshot e riaggiorno le variabili e vado avanti
    x += 1
    print(f"Errore: Pagina ({counter}) saltata")
    print(wd.current_url)
    wd.save_screenshot('screenshot.png') # Se vuoi salvarti gli screenshot devi var variare il nome del file...puoi inserire il conteggio dell'errore (x) nel nome del file con un printf
    %pylab inline
    import matplotlib.pyplot as plt
    import matplotlib.image as mpimg
    img=mpimg.imread('/content/screenshot.png')
    imgplot = plt.imshow(img)
    plt.show()

    wd.find_elements_by_css_selector("ul.pull-right a")[0].click()
    time.sleep(abs(1+np.random.randn()*0.2))
    counter = int(wd.find_element_by_css_selector("ul.pagination.pagination__number li.active a").text)
    continue
  for annuncio in lista_annunci:
    t += 1
    print(f"Pag {counter} Annuncio {t}/{len(lista_annunci)}")
    dettagli = {}
    id = annuncio.get_attribute("data-id")
    url = annuncio.find_elements_by_css_selector(f"#link_ad_{id}")[0].get_attribute("href")
    dettagli["id_annuncio"] = id
    dettagli["url"] = url
    dettagli["titolo"] = annuncio.find_elements_by_css_selector(f"#link_ad_{id}")[0].get_attribute("title")
    #print(f"Pag {counter} Annuncio {t}/{len(lista_annunci)} - {url}")

    wd2.get(url)
    time.sleep(abs(1+np.random.randn()*0.2))
    
    # Recupero dati
    try:
      pagina_annuncio = wd2.find_elements_by_css_selector("div.nd-grid.im-structure__container")[0]
      caratteristiche = []
      descrizione = []
      caratteristiche = pagina_annuncio.find_elements_by_css_selector("dt.im-features__title")
      descrizione = pagina_annuncio.find_elements_by_css_selector("dd.im-features__value")

      for i in range(0,len(caratteristiche)): 
        dettagli[caratteristiche[i].text]=descrizione[i].text
      dettagli["posizione"] =pagina_annuncio.find_elements_by_css_selector("div.im-map__description")[0].text
      dettagli["descrizione"] =pagina_annuncio.find_elements_by_css_selector("div.js-readAllText")[0].text
      dettagli["immobiliare"] =pagina_annuncio.find_elements_by_css_selector("div.im-lead__reference>a p")[0].text
    except:
      pass
    if len(dettagli) == 3: # Caso in cui non riesca a recuperare i dati nella pagina del singolo annuncio
      print("Something goes wrong :(")
      print(url)
      j+=1

    # Chiusura ciclo e salvataggio dati 
    dettagli_annunci.append(dettagli)

  # Aggiornamento variabili While
  wd.find_elements_by_css_selector("ul.pull-right a")[0].click()
  time.sleep(abs(1+np.random.randn()*0.2))
  try:
    counter = int(wd.find_element_by_css_selector("ul.pagination.pagination__number li.active a").text)
  except:
    print("Errore. Scraper terminato in anticipo. Salvataggio...")
    ds_dettagli_annunci = pd.DataFrame(dettagli_annunci)
    ds_dettagli_annunci.set_index("id_annuncio")
    ds_dettagli_annunci.head()
    ds_dettagli_annunci.info()
    ds_dettagli_annunci.to_csv(f'ds_annunci_{inizio}-{counter}.csv')
    sys.exit(0)
  if counter < totale : totale = int(wd.find_element_by_css_selector("#listing-pagination>ul.pagination__number li:last-child>a.disabled").text) # Aggiornamento costante pagina finale
  if counter == math.ceil(totale*n) :
    if inizio == math.ceil(totale*n) :
      counter += 1
    else: 
      y += 1
    if y==2:
      counter += 1
# Fine
str_annunci_totali = wd.find_element_by_css_selector("span.count-ads.hidden-xs").text
annunci_totali = int(str_annunci_totali.split(" ")[0].replace(".",""))
print(f"Estratti ({len(dettagli_annunci)}) Annunci su ({annunci_totali}) Annunci Totali - {j} Annunci difettosi - Pagine totali({totale}) - Pagine saltate ({x})")

# Save in csv file
ds_dettagli_annunci = pd.DataFrame(dettagli_annunci)
ds_dettagli_annunci.set_index("id_annuncio")
ds_dettagli_annunci.head()
ds_dettagli_annunci.info()
ds_dettagli_annunci.to_csv(f'ds_annunci_{inizio}-{math.ceil(totale*n)}.csv')
